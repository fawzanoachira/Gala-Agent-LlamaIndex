{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "docs = [\n",
    "    Document(\n",
    "        text=\"\\n\".join([\n",
    "            f\"Name: {guest_dataset['name'][i]}\",\n",
    "            f\"Relation: {guest_dataset['relation'][i]}\",\n",
    "            f\"Description: {guest_dataset['description'][i]}\",\n",
    "            f\"Email: {guest_dataset['email'][i]}\"\n",
    "        ]),\n",
    "        metadata={\"name\": guest_dataset['name'][i]}\n",
    "    )\n",
    "    for i in range(len(guest_dataset))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=docs)\n",
    "\n",
    "def get_guest_info_retriever(query: str) -> str:\n",
    "    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "    results = bm25_retriever.retrieve(query)\n",
    "    if results:\n",
    "        return \"\\n\\n\".join([doc.text for doc in results[:3]])\n",
    "    else:\n",
    "        return \"No matching guest information found.\"\n",
    "\n",
    "# Initialize the tool\n",
    "guest_info_tool = FunctionTool.from_defaults(get_guest_info_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown model: accounts/fireworks/models/llama4-scout-instruct-basic. Please provide a valid Fireworks model name.Known models are: accounts/fireworks/models/llama-v2-7b-chat, accounts/fireworks/models/llama-v2-13b-chat, accounts/fireworks/models/llama-v2-70b-chat, accounts/fireworks/models/llama-v2-34b-code-instruct, accounts/fireworks/models/llamaguard-7b, accounts/fireworks/models/llama-v3-8b-instruct, accounts/fireworks/models/llama-v3-70b-instruct, accounts/fireworks/models/llama-v3p1-8b-instruct, accounts/fireworks/models/llama-v3p1-70b-instruct, accounts/fireworks/models/llama-v3p1-405b-instruct, accounts/fireworks/models/llama-v3p2-1b-instruct, accounts/fireworks/models/llama-v3p2-3b-instruct, accounts/fireworks/models/llama-v3p2-11b-vision-instruct, accounts/fireworks/models/llama-v3p2-90b-vision-instruct, accounts/fireworks/models/mistral-7b-instruct-4k, accounts/fireworks/models/mixtral-8x7b-instruct, accounts/fireworks/models/firefunction-v1, accounts/fireworks/models/mixtral-8x22b-instruct, accounts/fireworks/models/firefunction-v2, accounts/fireworks/models/deepseek-v3, accounts/fireworks/models/deepseek-r1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m llm \u001b[38;5;241m=\u001b[39m Fireworks(\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccounts/fireworks/models/llama4-scout-instruct-basic\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mfw_api\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create Alfred, our gala agent, with the guest info tool\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m alfred \u001b[38;5;241m=\u001b[39m \u001b[43mAgentWorkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tools_or_functions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mguest_info_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Example query Alfred might receive during the gala\u001b[39;00m\n\u001b[1;32m     23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m alfred\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about our guest named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLady Ada Lovelace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/llama_index/core/agent/workflow/multi_agent_workflow.py:577\u001b[0m, in \u001b[0;36mAgentWorkflow.from_tools_or_functions\u001b[0;34m(cls, tools_or_functions, llm, system_prompt, state_prompt, initial_state, timeout, verbose)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initializes an AgentWorkflow from a list of tools or functions.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mThe workflow will be initialized with a single agent that uses the provided tools or functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03mOtherwise, it will use the ReActAgent.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39mllm\n\u001b[1;32m    576\u001b[0m agent_cls \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 577\u001b[0m     FunctionAgent \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241m.\u001b[39mis_function_calling_model \u001b[38;5;28;01melse\u001b[39;00m ReActAgent\n\u001b[1;32m    578\u001b[0m )\n\u001b[1;32m    580\u001b[0m tools \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    581\u001b[0m     FunctionTool\u001b[38;5;241m.\u001b[39mfrom_defaults(fn\u001b[38;5;241m=\u001b[39mtool)\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, BaseTool)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m tool\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools_or_functions\n\u001b[1;32m    585\u001b[0m ]\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    587\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    588\u001b[0m         agent_cls(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    600\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/llama_index/llms/fireworks/base.py:86\u001b[0m, in \u001b[0;36mFireworks.metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmetadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMMetadata:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMMetadata(\n\u001b[0;32m---> 86\u001b[0m         context_window\u001b[38;5;241m=\u001b[39m\u001b[43mfireworks_modelname_to_contextsize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     87\u001b[0m         num_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens,\n\u001b[1;32m     88\u001b[0m         is_chat_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     89\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     90\u001b[0m         is_function_calling_model\u001b[38;5;241m=\u001b[39mis_function_calling_model(\n\u001b[1;32m     91\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_name()\n\u001b[1;32m     92\u001b[0m         ),\n\u001b[1;32m     93\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/llama_index/llms/fireworks/utils.py:79\u001b[0m, in \u001b[0;36mfireworks_modelname_to_contextsize\u001b[0;34m(modelname)\u001b[0m\n\u001b[1;32m     76\u001b[0m context_size \u001b[38;5;241m=\u001b[39m ALL_AVAILABLE_MODELS\u001b[38;5;241m.\u001b[39mget(modelname, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please provide a valid Fireworks model name.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKnown models are: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ALL_AVAILABLE_MODELS\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context_size\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown model: accounts/fireworks/models/llama4-scout-instruct-basic. Please provide a valid Fireworks model name.Known models are: accounts/fireworks/models/llama-v2-7b-chat, accounts/fireworks/models/llama-v2-13b-chat, accounts/fireworks/models/llama-v2-70b-chat, accounts/fireworks/models/llama-v2-34b-code-instruct, accounts/fireworks/models/llamaguard-7b, accounts/fireworks/models/llama-v3-8b-instruct, accounts/fireworks/models/llama-v3-70b-instruct, accounts/fireworks/models/llama-v3p1-8b-instruct, accounts/fireworks/models/llama-v3p1-70b-instruct, accounts/fireworks/models/llama-v3p1-405b-instruct, accounts/fireworks/models/llama-v3p2-1b-instruct, accounts/fireworks/models/llama-v3p2-3b-instruct, accounts/fireworks/models/llama-v3p2-11b-vision-instruct, accounts/fireworks/models/llama-v3p2-90b-vision-instruct, accounts/fireworks/models/mistral-7b-instruct-4k, accounts/fireworks/models/mixtral-8x7b-instruct, accounts/fireworks/models/firefunction-v1, accounts/fireworks/models/mixtral-8x22b-instruct, accounts/fireworks/models/firefunction-v2, accounts/fireworks/models/deepseek-v3, accounts/fireworks/models/deepseek-r1"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.llms.fireworks import Fireworks\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "fw_api = os.getenv(\"fw_api\")\n",
    "# Initialize the Hugging Face model\n",
    "# llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "llm = Fireworks(\n",
    "    model=\"accounts/fireworks/models/llama4-scout-instruct-basic\", api_key=fw_api\n",
    ")\n",
    "\n",
    "# Create Alfred, our gala agent, with the guest info tool\n",
    "alfred = AgentWorkflow.from_tools_or_functions(\n",
    "    [guest_info_tool],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Example query Alfred might receive during the gala\n",
    "response = await alfred.run(\"Tell me about our guest named 'Lady Ada Lovelace'.\")\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was the Mayor of Tulle from 2001 to 2008, and was the President of the CorrÃ¨ze General Council from 2008 to 2012. He was elected as President of France on 6 May 2012, defeating the incumbent Nicolas Sarkozy. Hollande was born in Rouen, Seine-Maritime, Upper Normandy, to a middle-class family.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# Initialize the DuckDuckGo search tool\n",
    "tool_spec = DuckDuckGoSearchToolSpec()\n",
    "\n",
    "search_tool = FunctionTool.from_defaults(tool_spec.duckduckgo_full_search)\n",
    "# Example usage\n",
    "response = search_tool(\"Who's the current President of France?\")\n",
    "print(response.raw_output[-1]['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def get_weather_info(location: str) -> str:\n",
    "    \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
    "    # Dummy weather data\n",
    "    weather_conditions = [\n",
    "        {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
    "        {\"condition\": \"Clear\", \"temp_c\": 25},\n",
    "        {\"condition\": \"Windy\", \"temp_c\": 20},\n",
    "    ]\n",
    "    # Randomly select a weather condition\n",
    "    data = random.choice(weather_conditions)\n",
    "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C\"\n",
    "\n",
    "\n",
    "# Initialize the tool\n",
    "weather_info_tool = FunctionTool.from_defaults(get_weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most downloaded model by facebook is facebook/esmfold_v1 with 22,216,629 downloads.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from huggingface_hub import list_models\n",
    "\n",
    "\n",
    "def get_hub_stats(author: str) -> str:\n",
    "    \"\"\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\"\"\n",
    "    try:\n",
    "        # List models from the specified author, sorted by downloads\n",
    "        models = list(\n",
    "            list_models(author=author, sort=\"downloads\", direction=-1, limit=1)\n",
    "        )\n",
    "\n",
    "        if models:\n",
    "            model = models[0]\n",
    "            return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
    "        else:\n",
    "            return f\"No models found for author {author}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching models for {author}: {str(e)}\"\n",
    "\n",
    "\n",
    "# Initialize the tool\n",
    "hub_stats_tool = FunctionTool.from_defaults(get_hub_stats)\n",
    "\n",
    "# Example usage\n",
    "print(hub_stats_tool(\"facebook\"))  # Example: Get the most downloaded model by Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ© Alfred's Response:\n",
      "<think>\n",
      "Okay, the user is asking two things: what Facebook is and what their most popular model is. Let me break this down.\n",
      "\n",
      "First, I need to explain what Facebook is. I know Facebook is a social media company, now part of Meta. But maybe I should check if there's more up-to-date information. Wait, the user might be referring to Facebook the company or the platform. Since they mentioned \"their most popular model,\" maybe they mean a machine learning model developed by Facebook's AI team. Like maybe something from their AI research division, FAIR.\n",
      "\n",
      "So for the first part, I can briefly describe Facebook as a social media platform and part of Meta. Then, for the most popular model, I recall that Facebook has developed models like LLaMA (Large Language Model Meta AI), which is quite popular. But I should verify this. Alternatively, maybe they have other models like RoBERTa or DETR. But LLaMA has been widely discussed recently. However, I need to confirm if that's the most popular.\n",
      "\n",
      "I can use the get_hub_stats tool to check the most downloaded model from a specific author. The author here would be \"facebook\" or \"meta\". Let me try using the get_hub_stats tool\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "# Create Alfred with all the tools\n",
    "alfred = AgentWorkflow.from_tools_or_functions(\n",
    "    [search_tool, weather_info_tool, hub_stats_tool], llm=llm\n",
    ")\n",
    "\n",
    "# Example query Alfred might receive during the gala\n",
    "response = await alfred.run(\"What is Facebook and what's their most popular model?\")\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
