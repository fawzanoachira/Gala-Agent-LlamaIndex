{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waib3/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "docs = [\n",
    "    Document(\n",
    "        text=\"\\n\".join([\n",
    "            f\"Name: {guest_dataset['name'][i]}\",\n",
    "            f\"Relation: {guest_dataset['relation'][i]}\",\n",
    "            f\"Description: {guest_dataset['description'][i]}\",\n",
    "            f\"Email: {guest_dataset['email'][i]}\"\n",
    "        ]),\n",
    "        metadata={\"name\": guest_dataset['name'][i]}\n",
    "    )\n",
    "    for i in range(len(guest_dataset))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=docs)\n",
    "\n",
    "def get_guest_info_retriever(query: str) -> str:\n",
    "    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "    results = bm25_retriever.retrieve(query)\n",
    "    if results:\n",
    "        return \"\\n\\n\".join([doc.text for doc in results[:3]])\n",
    "    else:\n",
    "        return \"No matching guest information found.\"\n",
    "\n",
    "# Initialize the tool\n",
    "guest_info_tool = FunctionTool.from_defaults(get_guest_info_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/llama_index/core/workflow/context.py:583\u001b[0m, in \u001b[0;36mContext._step_worker\u001b[0;34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, dispatcher)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m     new_ev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    584\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:368\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/llama_index/core/agent/workflow/multi_agent_workflow.py:382\u001b[0m, in \u001b[0;36mAgentWorkflow.run_agent_step\u001b[0;34m(self, ctx, ev)\u001b[0m\n\u001b[1;32m    380\u001b[0m tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tools(ev\u001b[38;5;241m.\u001b[39mcurrent_agent_name, user_msg_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 382\u001b[0m agent_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mtake_step(\n\u001b[1;32m    383\u001b[0m     ctx,\n\u001b[1;32m    384\u001b[0m     ev\u001b[38;5;241m.\u001b[39minput,\n\u001b[1;32m    385\u001b[0m     tools,\n\u001b[1;32m    386\u001b[0m     memory,\n\u001b[1;32m    387\u001b[0m )\n\u001b[1;32m    389\u001b[0m ctx\u001b[38;5;241m.\u001b[39mwrite_event_to_stream(agent_output)\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/llama_index/core/agent/workflow/react_agent.py:99\u001b[0m, in \u001b[0;36mReActAgent.take_step\u001b[0;34m(self, ctx, llm_input, tools, memory)\u001b[0m\n\u001b[1;32m     98\u001b[0m last_chat_response \u001b[38;5;241m=\u001b[39m ChatResponse(message\u001b[38;5;241m=\u001b[39mChatMessage())\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m last_chat_response \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[1;32m    100\u001b[0m     raw \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    101\u001b[0m         last_chat_response\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_chat_response\u001b[38;5;241m.\u001b[39mraw, BaseModel)\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m last_chat_response\u001b[38;5;241m.\u001b[39mraw\n\u001b[1;32m    104\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/llama_index/llms/huggingface_api/base.py:429\u001b[0m, in \u001b[0;36mHuggingFaceInferenceAPI.astream_chat.<locals>.gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    428\u001b[0m cur_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_client\u001b[38;5;241m.\u001b[39mchat_completion(\n\u001b[1;32m    430\u001b[0m     messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_huggingface_messages(messages),\n\u001b[1;32m    431\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    433\u001b[0m ):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfinish_reason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/huggingface_hub/inference/_generated/_async_client.py:1032\u001b[0m, in \u001b[0;36mAsyncInferenceClient.chat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m   1025\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[1;32m   1026\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   1027\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[1;32m   1031\u001b[0m )\n\u001b[0;32m-> 1032\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_post(request_parameters, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/huggingface_hub/inference/_generated/_async_client.py:367\u001b[0m, in \u001b[0;36mAsyncInferenceClient._inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m session\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/huggingface_hub/inference/_generated/_async_client.py:353\u001b[0m, in \u001b[0;36mAsyncInferenceClient._inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/aiohttp/client_reqrep.py:1161\u001b[0m, in \u001b[0;36mClientResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m-> 1161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[1;32m   1164\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m   1165\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m   1166\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1167\u001b[0m )\n",
      "\u001b[0;31mClientResponseError\u001b[0m: 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mWorkflowRuntimeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m alfred \u001b[38;5;241m=\u001b[39m AgentWorkflow\u001b[38;5;241m.\u001b[39mfrom_tools_or_functions(\n\u001b[1;32m      9\u001b[0m     [guest_info_tool],\n\u001b[1;32m     10\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Example query Alfred might receive during the gala\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m alfred\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about our guest named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLady Ada Lovelace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎩 Alfred\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/llama_index/core/workflow/workflow.py:394\u001b[0m, in \u001b[0;36mWorkflow.run.<locals>._run_workflow\u001b[0;34m()\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mwrite_event_to_stream(StopEvent())\n\u001b[0;32m--> 394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[1;32m    398\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mwrite_event_to_stream(StopEvent())\n",
      "File \u001b[0;32m~/Documents/m/Gala-Agent-LlamaIndex/.venv/lib/python3.10/site-packages/llama_index/core/workflow/context.py:592\u001b[0m, in \u001b[0;36mContext._step_worker\u001b[0;34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, dispatcher)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mretry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[1;32m    593\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in step \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    594\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     delay \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mretry_policy\u001b[38;5;241m.\u001b[39mnext(\n\u001b[1;32m    597\u001b[0m         retry_start_at \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mtime(), attempts, e\n\u001b[1;32m    598\u001b[0m     )\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[0;31mWorkflowRuntimeError\u001b[0m: Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "# Initialize the Hugging Face model\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# Create Alfred, our gala agent, with the guest info tool\n",
    "alfred = AgentWorkflow.from_tools_or_functions(\n",
    "    [guest_info_tool],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Example query Alfred might receive during the gala\n",
    "response = await alfred.run(\"Tell me about our guest named 'Lady Ada Lovelace'.\")\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
